{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_index = \"https://myindex.jp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_urls(url):\n",
    "    res = req.urlopen(url)\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    div = soup.find('div', id='tab_searchf_1')\n",
    "    table = div.find('table', class_='tabstyle')\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    # tableのlink先取得\n",
    "    link_urls = []\n",
    "    for row in rows:\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) >= 3:\n",
    "            link = columns[2].find(\"a\")\n",
    "            if link:\n",
    "                href = link[\"href\"]\n",
    "                link_url = url_index + href\n",
    "                link_urls.append(link_url)\n",
    "\n",
    "    return link_urls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://myindex.jp/data_f.php?q=1319',\n",
       " 'https://myindex.jp/data_f.php?q=1364',\n",
       " 'https://myindex.jp/data_f.php?q=1474',\n",
       " 'https://myindex.jp/data_f.php?q=1551',\n",
       " 'https://myindex.jp/data_f.php?q=1591',\n",
       " 'https://myindex.jp/data_f.php?q=1592',\n",
       " 'https://myindex.jp/data_f.php?q=1593',\n",
       " 'https://myindex.jp/data_f.php?q=1599',\n",
       " 'https://myindex.jp/data_f.php?q=2042',\n",
       " 'https://myindex.jp/data_f.php?q=2516',\n",
       " 'https://myindex.jp/data_f.php?q=2526',\n",
       " 'https://myindex.jp/data_f.php?q=DMJP',\n",
       " 'https://myindex.jp/data_f.php?q=EWJ',\n",
       " 'https://myindex.jp/data_f.php?q=01311143',\n",
       " 'https://myindex.jp/data_f.php?q=0131A141',\n",
       " 'https://myindex.jp/data_f.php?q=02317141',\n",
       " 'https://myindex.jp/data_f.php?q=03311144',\n",
       " 'https://myindex.jp/data_f.php?q=04313141',\n",
       " 'https://myindex.jp/data_f.php?q=04314141',\n",
       " 'https://myindex.jp/data_f.php?q=04316141']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_urls = get_link_urls(\"https://myindex.jp/search_fund.php\")\n",
    "link_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEXT FUNDS 日経300株価指数連動型上場投資信託 (1319)', '日経株価指数300', '日本株式', '0.52%\\u3000(年率・税抜)'] ['+6.6', '16.8', '0.4']\n"
     ]
    }
   ],
   "source": [
    "def get_attibute(soup):\n",
    "    attribute = []\n",
    "    div = soup.find('div', id='indexinfo_gaiyo')\n",
    "    table = div.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    for row in rows:\n",
    "        row_text = [td.get_text(strip=True) for td in row.select('th, td')]\n",
    "        if any(keyword in row_text[0] for keyword in [\"銘柄名\", \"大カテゴリ\", \"連動インデックス\", \"信託報酬\"]):\n",
    "            attribute.append(row_text[1])\n",
    "    return attribute\n",
    "\n",
    "\n",
    "def get_performance(soup):\n",
    "    performance = []\n",
    "    div = soup.find('div', class_='indexinfo_pfm w99')\n",
    "    table = div.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    # リスク、リターン、シャープレシオを抽出\n",
    "    for row in rows:\n",
    "        if row.find('td'):\n",
    "            row_text = [td.get_text(strip=True) for td in row.select('td')]\n",
    "            if any(keyword in row_text[0] for keyword in [\"リスク\", \"リターン\", \"シャープレシオ\"]):\n",
    "                # print(row_text)\n",
    "                # 最長期間のデータのみを抽出\n",
    "                for i, elm in enumerate(reversed(row_text)):\n",
    "                    if elm:\n",
    "                        performance.append(elm)\n",
    "                        break\n",
    "                    if i >6:      # 過去データが1年以上ある銘柄のみを抽出\n",
    "                        break\n",
    "    return performance\n",
    "\n",
    "\n",
    "for link_url in link_urls:\n",
    "    response = req.urlopen(link_url)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    \n",
    "    attribute = get_attibute(soup)\n",
    "    performance = get_performance(soup)\n",
    "    print(attribute, performance)\n",
    "    \n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+6.6', '16.8', '0.4']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comb_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
